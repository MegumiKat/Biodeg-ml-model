# -*- coding: utf-8 -*-
"""Final verison- Biodeg modelling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ASt_f1hl_6iKhsVW-OCw6LAyvG_dE84P
"""

#!!!! Random Forest with all the data from Excel

import pandas as pd
import numpy as np   # numerical calcul
from sklearn.model_selection import train_test_split  # divided in train and test split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, classification_report  # to evaluate the model
from sklearn.metrics import r2_score # give the r2 of the model

# Lire le fichier Excel
df = pd.read_excel("Final version-Biodeg data summary sheet.xlsx")
df.rename(columns={'Composition (%)': 'CHDA', 'Unnamed: 2': 'EG', 'Unnamed: 3': 'SSIA', 'Unnamed: 4': 'SSIT' , 'Unnamed: 5': 'PDA' , 'Unnamed: 6': 'Quat-PDA' , 'Unnamed: 7': 'PET', 'Unnamed: 8': 'LA' , 'Unnamed: 9': 'TMA'}, inplace=True)
df.rename(columns={'Unnamed: 10': 'CHDM', 'Unnamed: 11': 'FUGLA', 'Unnamed: 12': 'CHGLA', 'Unnamed: 13': 'BHET' , 'Unnamed: 14': 'NPG' , 'Unnamed: 15': 'TA' , 'Unnamed: 16': 'FDCA', 'Unnamed: 17': 'FDME' , 'Unnamed: 18': 'Ad'}, inplace=True)
df.rename(columns={'301F test result\n(28D)': '301F test result 30D', '301F test result\n(60D)': '301F test result 60D'}, inplace=True)

# Supprimer les lignes inutiles (les 3 suivantes après la ligne fusionnée)
df = df.drop(index=[0])


#df.fillna({"Calories": 0}, inplace=True) # replace NaN by 0 in the selected columns (possible to put many columns in one code line ?)
df.fillna({"CHDA": 0, "EG": 0, "SSIA":0, "SSIT":0, "PDA":0, "Quat-PDA":0, "PET": 0, "LA": 0, "TMA": 0, "CHDM": 0, "FUGLA": 0, "CHGLA": 0, "BHET": 0, "NPG": 0, "TA": 0, "FDCA": 0, "FDME": 0, "Ad": 0}, inplace=True)
print(df)
print("Which biodegradability do you want to predict ? 302B, 301F30D or 301F60D")
choice = input()

if choice == "302B" or choice == "302b":
  df = df.dropna(subset=['302B test result'])  #or '302B_test_result'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["302B test result"]
elif choice == "301F30D" or choice == "301f30d" or choice == "301F30d" or choice == "301f30D" or choice == "30d" or choice == "30D":
  df = df.dropna(subset=['301F test result 30D'])  #or '301F_test_result_30D'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["301F test result 30D"]
elif choice == "301F60D" or choice == "301f60d" or choice == "301F60d" or choice == "301f60D" or choice == "60d" or choice == "60D" :
  df = df.dropna(subset=['301F test result 60D'])  #or '301F_test_result_60D'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["301F test result 60D"]
else :
  print("please choose a name among the 3 possibilities or be sure to write it as the same way as in the previous list")
  #exit()  #it is really useful to crash the program since there is no action after ?

X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25, random_state=42) # creation of the different set #X.values remove the warning so it's perfect

print('size of train set:', X_train.shape)
print('size of test set:', X_test.shape)

# Random Forest
rf = RandomForestRegressor(100)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
print('accuracy of the tree is', r2_score(y_test, y_pred)*100)


# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print('accuracy of the linear regression is', r2_score(y_test, y_pred_lr)*100)


# Ridge Regression
rl = Ridge(alpha=0.5)
rl.fit(X_train, y_train)
y_pred_rl = rl.predict(X_test)
print('accuracy of the ridge regression is', r2_score(y_test, y_pred_rl)*100)


# Lasso Regression
ll = Lasso(alpha=0.5)
ll.fit(X_train, y_train)
y_pred_ll = ll.predict(X_test)
print('accuracy of the lasso regression is', r2_score(y_test, y_pred_ll)*100)



# Elastic Net Regression
en = ElasticNet(alpha=0.5)
en.fit(X_train, y_train)
y_pred_en = en.predict(X_test)
print('accuracy of the elastic net regression is', r2_score(y_test, y_pred_en)*100)



# Support Vector Regression
svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)
svr.fit(X_train, y_train)
y_pred_svr = svr.predict(X_test)
print('accuracy of the svr is', r2_score(y_test, y_pred_svr)*100)



# K-Nearest Neighbors Regression
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print('accuracy of the knn is', r2_score(y_test, y_pred_knn)*100)



# Decision Tree Regression
dt = DecisionTreeRegressor(max_depth=5)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
print('accuracy of the decision tree is', r2_score(y_test, y_pred_dt)*100)



# Gradient Boosting Regression
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
print('accuracy of the gradient boosting is', r2_score(y_test, y_pred_gb)*100)



# Multi-Layer Perceptron Regression
mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000)
mlp.fit(X_train, y_train)
y_pred_mlp = mlp.predict(X_test)
print('accuracy of the mlp is', r2_score(y_test, y_pred_mlp)*100)



# Gaussian Process Regression
gp = GaussianProcessRegressor(kernel=RBF(length_scale=1.0), alpha=1e-6)
gp.fit(X_train, y_train)
y_pred_gp = gp.predict(X_test)
print('accuracy of the gaussian process is', r2_score(y_test, y_pred_gp)*100)



# PCA + Random Forest
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 执行 PCA，保留前 2 个主成分（你也可以尝试更多）
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 划分训练测试集（PCA之后的）
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.25, random_state=42)

# 随机森林回归模型训练（使用PCA降维后的数据）
rf_pca = RandomForestRegressor(100)
rf_pca.fit(X_train_pca, y_train_pca)
y_pred_pca = rf_pca.predict(X_test_pca)

# 输出精度
print('accuracy of the Random Forest after PCA (2D) is', r2_score(y_test_pca, y_pred_pca)*100)

# 可视化：PCA 降维后样本分布（可选）
# import seaborn as sns
# plt.figure(figsize=(8,6))
# sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='coolwarm')
# plt.xlabel('Principal Component 1')
# plt.ylabel('Principal Component 2')
# plt.title('PCA Projection (2D)')
# plt.grid(True)
# plt.tight_layout()
# plt.show()



# 10-Fold Cross Validation R² Comparison
models = {
    "Random Forest": RandomForestRegressor(n_estimators=100),
    "Linear Regression": LinearRegression(),
    "Ridge": Ridge(alpha=0.5),
    "Lasso": Lasso(alpha=0.5),
    "ElasticNet": ElasticNet(alpha=0.5),
    "SVR": SVR(kernel='rbf', C=1.0, epsilon=0.1),
    "KNN": KNeighborsRegressor(n_neighbors=5),
    "Decision Tree": DecisionTreeRegressor(max_depth=5),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5),
    "MLP": MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000),
    "Gaussian Process": GaussianProcessRegressor(kernel=RBF(length_scale=1.0), alpha=1e-6)
}

print("\n =================== 10-Fold Cross Validation R² Comparison: ===================\n")

results = []

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    print(f"{name:<20} | Mean R²: {mean_score:.4f} | Std: {std_score:.4f}")
    results.append((name, mean_score, std_score))


print('et voila')
print((rf.predict([[36.3,45.45,4.54,0,4.54,0,0,9.09,0,0,0,0,0,0,0,0,0,0]]))*100)

#print(df)
#print(df.columns)

#!!!! Random Forest with all the data from Excel with manual enter for each data !!!!!!

import pandas as pd
import numpy as np   # numerical calcul
from sklearn.model_selection import train_test_split  # divided in train and test split
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, classification_report  # to evaluate the model
from sklearn.metrics import r2_score # give the r2 of the model

# Lire le fichier Excel
df = pd.read_excel("Final version-Biodeg data summary sheet.xlsx")
df.rename(columns={'Composition (%)': 'CHDA', 'Unnamed: 2': 'EG', 'Unnamed: 3': 'SSIA', 'Unnamed: 4': 'SSIT' , 'Unnamed: 5': 'PDA' , 'Unnamed: 6': 'Quat-PDA' , 'Unnamed: 7': 'PET', 'Unnamed: 8': 'LA' , 'Unnamed: 9': 'TMA'}, inplace=True)
df.rename(columns={'Unnamed: 10': 'CHDM', 'Unnamed: 11': 'FUGLA', 'Unnamed: 12': 'CHGLA', 'Unnamed: 13': 'BHET' , 'Unnamed: 14': 'NPG' , 'Unnamed: 15': 'TA' , 'Unnamed: 16': 'FDCA', 'Unnamed: 17': 'FDME' , 'Unnamed: 18': 'Ad'}, inplace=True)
df.rename(columns={'301F test result\n(28D)': '301F test result 30D', '301F test result\n(60D)': '301F test result 60D'}, inplace=True)

# Supprimer les lignes inutiles (les 3 suivantes après la ligne fusionnée)
df = df.drop(index=[0])

#df.fillna({"Calories": 0}, inplace=True) # replace NaN by 0 in the selected columns (possible to put many columns in one code line ?)
df.fillna({"CHDA": 0, "EG": 0, "SSIA":0, "SSIT":0, "PDA":0, "Quat-PDA":0, "PET": 0, "LA": 0, "TMA": 0, "CHDM": 0, "FUGLA": 0, "CHGLA": 0, "BHET": 0, "NPG": 0, "TA": 0, "FDCA": 0, "FDME": 0, "Ad": 0}, inplace=True)

print("Which biodegradability do you want to predict ? 302B, 301F30D or 301F60D")
choice = input()

if choice == "302B" or choice == "302b":
  df = df.dropna(subset=['302B test result'])  #or '302B_test_result'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["302B test result"]
elif choice == "301F30D" or choice == "301f30d" or choice == "301F30d" or choice == "301f30D" :
  df = df.dropna(subset=['301F test result 30D'])  #or '301F_test_result_30D'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["301F test result 30D"]
elif choice == "301F60D" or choice == "301f60d" or choice == "301F60d" or choice == "301f60D" :
  df = df.dropna(subset=['301F test result 60D'])  #or '301F_test_result_60D'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["301F test result 60D"]
else :
  print("please choose a name among the 3 possibilities or be sure to write it as the same way as in the previous list")
  #exit()  #it is really useful to crash the program since there is no action after ?

A = float(input("% of CHDA :"))
B = float(input("% of EG :" ))
C = float(input("% of SSIA :"))
D = float(input("% of SSIT :"))
E = float(input("% of PDA :"))
F = float(input("% of Quat-PDA :"))
G = float(input("% of PET :"))
H = float(input("% of LA :"))
I = float(input("% of TMA :"))
J = float(input("% of CHDM :"))
K = float(input("% of FUGLA :"))
L = float(input("% of CHGLA :"))
M = float(input("% of BHET :"))
N = float(input("% of NPG :"))
O = float(input("% of TA :"))
P = float(input("% of FDCA :"))
Q = float(input("% of FDME :"))
R = float(input("% of Ad :"))

check = A+B+C+D+E+F+G+H+I+J+K+L+M+N+O+P+Q+R
if check == 100 :
  X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25, random_state=42) # creation of the different set #X.values remove the warning so it's perfect
  print('size of train set:', X_train.shape)
  print('size of test set:', X_test.shape)




  rf = RandomForestRegressor(100)
  lr = LinearRegression()

  rf.fit(X_train, y_train)
  lr.fit(X_train, y_train)

  y_pred = rf.predict(X_test)
  y_pred_lr = lr.predict(X_test)

  print('accuracy of the linear regression is', r2_score(y_test, y_pred_lr)*100)
  print('accuracy of the tree is', r2_score(y_test, y_pred)*100)
  #!!!!!!!!!!! Place a la prediction
  print('et voila')
  print(rf.predict([[A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R]]))
else:
    print("the sum of % is:", check)
    print("please enter again the data and be sure the sum is 100")

#!!!! Random Forest with all the data from Excel manual enter a general one for all the data !!!!

import pandas as pd
import numpy as np   # numerical calcul
from sklearn.model_selection import train_test_split  # divided in train and test split
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, classification_report  # to evaluate the model
from sklearn.metrics import r2_score # give the r2 of the model

# Lire le fichier Excel
df = pd.read_excel("Final version-Biodeg data summary sheet.xlsx")
df.rename(columns={'Composition (%)': 'CHDA', 'Unnamed: 2': 'EG', 'Unnamed: 3': 'SSIA', 'Unnamed: 4': 'SSIT' , 'Unnamed: 5': 'PDA' , 'Unnamed: 6': 'Quat-PDA' , 'Unnamed: 7': 'PET', 'Unnamed: 8': 'LA' , 'Unnamed: 9': 'TMA'}, inplace=True)
df.rename(columns={'Unnamed: 10': 'CHDM', 'Unnamed: 11': 'FUGLA', 'Unnamed: 12': 'CHGLA', 'Unnamed: 13': 'BHET' , 'Unnamed: 14': 'NPG' , 'Unnamed: 15': 'TA' , 'Unnamed: 16': 'FDCA', 'Unnamed: 17': 'FDME' , 'Unnamed: 18': 'Ad'}, inplace=True)
df.rename(columns={'301F test result\n(28D)': '301F test result 30D', '301F test result\n(60D)': '301F test result 60D'}, inplace=True)

# Supprimer les lignes inutiles (les 3 suivantes après la ligne fusionnée)
df = df.drop(index=[0])

#df.fillna({"Calories": 0}, inplace=True) # replace NaN by 0 in the selected columns (possible to put many columns in one code line ?)
df.fillna({"CHDA": 0, "EG": 0, "SSIA":0, "SSIT":0, "PDA":0, "Quat-PDA":0, "PET": 0, "LA": 0, "TMA": 0, "CHDM": 0, "FUGLA": 0, "CHGLA": 0, "BHET": 0, "NPG": 0, "TA": 0, "FDCA": 0, "FDME": 0, "Ad": 0}, inplace=True)

print("Which biodegradability do you want to predict ? 302B, 301F30D or 301F60D")
choice = input()

if choice == "302B" or choice == "302b":
  df = df.dropna(subset=['302B test result'])  #or '302B_test_result'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["302B test result"]
elif choice == "301F30D" or choice == "301f30d" or choice == "301F30d" or choice == "301f30D" :
  df = df.dropna(subset=['301F test result 30D'])  #or '301F_test_result_30D'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["301F test result 30D"]
elif choice == "301F60D" or choice == "301f60d" or choice == "301F60d" or choice == "301f60D" :
  df = df.dropna(subset=['301F test result 60D'])  #or '301F_test_result_60D'
  features = ["CHDA","EG","SSIA","SSIT","PDA","Quat-PDA","PET","LA","TMA","CHDM","FUGLA","CHGLA","BHET","NPG","TA","FDCA","FDME","Ad"]
  X = df[features]
  y = df["301F test result 60D"]
else :
  print("please choose a name among the 3 possibilities or be sure to write it as the same way as in the previous list")
  #exit()  #it is really useful to crash the program since there is no action after ?

#entrees = list(map(int, input("enter the % in the right order and separate the number with a comma, : ").split(',')))
entrees = list(map(float, input("enter the % in the right order and separate the number with a comma, : ").split(',')))
#print(entrees)
check = len(entrees)
#print(check)

if check==18:
  somme = sum(entrees)
  #print(somme)
  if somme==100:
      X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25, random_state=42) # creation of the different set #X.values remove the warning so it's perfect
      print('size of train set:', X_train.shape)
      print('size of test set:', X_test.shape)


      rf = RandomForestRegressor(100)
      lr = LinearRegression()

      rf.fit(X_train, y_train)
      lr.fit(X_train, y_train)

      y_pred_lr = lr.predict(X_test)
      y_pred = rf.predict(X_test)
      print('accuracy of the tree is', r2_score(y_test, y_pred)*100)
      print('accuracy of the linear regression is', r2_score(y_test, y_pred_lr)*100)
      #!!!!!!!!!!! Place a la prediction
      print('et voila')
      print(rf.predict([entrees])*100)
  else:
      print("total is not 100 % please try again")
else:
  print("the number of components is not 18 please try again")